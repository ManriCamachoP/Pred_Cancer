---
title: "Untitled"
output: html_document
date: "2024-06-27"
---

# Implementation Boosting

## Overview

## Code Implementation

```{r}
pacman::p_load(
  # Data manipulation
  tidyverse,
  
  #Tree:
  magrittr, rio, here, caret, kknn, performance, pROC, class, randomForest,
  
  # Other:
  ggthemes
)
```


```{r}

```


### Import the Data

```{r}
df = read.csv("~/Library/CloudStorage/OneDrive-UniversidaddeCostaRica/GitHub/PrediccioÌn_Cancer/data/processed/data.csv")[,-c(1,2)]
head(df)
```

#### Transform to the correct datatypes

```{r}
df = df %>% 
  mutate_at(.vars = vars(Race, Education, 
                         Coverage, GeneralHealth, 
                         Children, Depression,
                         Exercise, Smoker,
                         Mammo, Breast), 
            .funs = as.factor)

df$Breast = relevel(x = df$Breast, ref = "No Breast Cancer")
```

#### Splitting the Dataset

We'll be applying 10-fold for cross validation

1.  Apply cross validation to see the optimal decision boundary
    1.  We'll apply the following hyperparameters to compare . Where if the y_predicted is bigger than the respective decision boundary will predict 1 \~ Breast Cancer

```{r}
folds = function(df, k, seed){
  
  set.seed(seed = seed)
  
  fold_group = sample(rep(1:k, length.out = nrow(df)))
  
  df$fold = fold_group
  
  return(df)
  
}
```

```{r}
df_folds = folds(df, k = 10, seed = 8)
```

### Implementing Random Forest Method

```{r}
boosting_model = function(df_folds, n_test_fold, m_final, coeflearn){
  
  boosting_model = boosting(data = df_folds[df_folds$fold != n_test_fold,-15],
                          Breast ~., m_final = m_final, boos = TRUE, coeflearn = coeflearn, 
                          )

  predictions = predict(rf_model, df_folds[df_folds$fold == n_test_fold,-c(14,15)])$class
  
  predictions = ifelse(predictions=="No Breast Cancer", yes = 0, no = 1)
  
  return(factor(predictions, levels = c(0, 1), labels = c("No Breast Cancer", "Breast Cancer")))
  
}
```

```{r}
performance_logistic = function(y_pred, y_test){
  
  cm = confusionMatrix(y_test, y_pred ,positive = "Breast Cancer")
  
  results = c(
    cm$overall['Accuracy'],
    1 - cm$overall['Accuracy'], 
    cm$byClass['Sensitivity'], 
    cm$byClass['Specificity'],
    fp = cm$table[1,2]/(cm$table[1,2]+cm$table[1,1]), 
    fn = cm$table[2,1]/(cm$table[2,1]+cm$table[2,2]), 
    cm$overall['Kappa'],
    auc(roc(as.numeric(y_test), as.numeric(y_pred)))
  )
  
  return(results)
  
}
```

```{r}
boosting_validation = function(df_folds, m_final, coeflearn){
  
  final_df = matrix(data = NA,
                    nrow = max(df_folds$fold), ncol = 10)
  
  for (fold in 1:max(df_folds$fold)){
    
    predictions = boosting_model(df_folds = df_folds, 
                            n_test_fold = fold, 
                            m_final = m_final, 
                            coeflearn = coeflearn)
    
    results = performance_logistic(y_pred = predictions, 
                                   y_test = df_folds$Breast[df_folds$fold == fold])
    
    results = c(fold, results, m_final, coeflearn)
    
    final_df[fold,] = results
  
  }
  
  final_df = as.data.frame(final_df)
  colnames(final_df) = c('Test Fold', "Accuracy", "Precision",
                         "Sensitivity", "Specificity", "Fake_Positives",
                         "Fake_Negatives", "Kappa", "auc", 'm_final', 'coeflearn')
  
  
  return(final_df)
}
```

```{r}
boosting_validation(df_folds = df_folds, m_final = 50, coeflearn = 0.01)
```

